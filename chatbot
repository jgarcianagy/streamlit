import streamlit as st
import os

# Intentar importar google-genai, si falla usar google-generativeai
try:
    from google import genai
    from google.genai import types
    USE_VERTEX = True
except ImportError:
    import google.generativeai as genai
    USE_VERTEX = False
    st.warning("‚ö†Ô∏è Usando google-generativeai en lugar de google-genai. Algunas funciones pueden diferir.")

import base64

# Configuraci√≥n de la p√°gina
st.set_page_config(
    page_title="Chatbot Oferta Pro a Pro",
    page_icon="ü§ñ",
    layout="wide"
)

# Funci√≥n para inicializar el cliente de Gemini
@st.cache_resource
def init_gemini_client():
    """Inicializa el cliente de Gemini con configuraci√≥n cached"""
    try:
        if USE_VERTEX:
            client = genai.Client(
                vertexai=True,
                project="cf-esproapro-bic-pro-ou",
                location="europe-central2",
            )
            return client
        else:
            # Para google-generativeai necesitas configurar la API key
            # genai.configure(api_key="tu_api_key_aqui")
            st.error("Para usar google-generativeai necesitas configurar tu API key")
            return None
    except Exception as e:
        st.error(f"Error al inicializar el cliente de Gemini: {str(e)}")
        return None

# Funci√≥n para obtener la configuraci√≥n del modelo
def get_model_config():
    """Retorna la configuraci√≥n del modelo"""
    if USE_VERTEX:
        tools = [
            types.Tool(
                retrieval=types.Retrieval(
                    vertex_rag_store=types.VertexRagStore(
                        rag_resources=[
                            types.VertexRagStoreRagResource(
                                rag_corpus="projects/503088742824/locations/europe-west3/ragCorpora/6917529027641081856"
                            )
                        ],
                        similarity_top_k=20,
                    )
                )
            )
        ]

        config = types.GenerateContentConfig(
            temperature=1,
            top_p=0.95,
            max_output_tokens=65535,
            safety_settings=[
                types.SafetySetting(
                    category="HARM_CATEGORY_HATE_SPEECH",
                    threshold="OFF"
                ),
                types.SafetySetting(
                    category="HARM_CATEGORY_DANGEROUS_CONTENT",
                    threshold="OFF"
                ),
                types.SafetySetting(
                    category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    threshold="OFF"
                ),
                types.SafetySetting(
                    category="HARM_CATEGORY_HARASSMENT",
                    threshold="OFF"
                )
            ],
            tools=tools,
            system_instruction=[
                types.Part.from_text(
                    text="Enf√≥cate el contexto que te aporto en tu corpus rag. siempre enfoca tus respuesta a este contexto.  "
                )
            ]
        )
    else:
        # Configuraci√≥n b√°sica para google-generativeai
        config = {
            'temperature': 1,
            'top_p': 0.95,
            'max_output_tokens': 65535,
        }
    
    return config

# Funci√≥n para generar respuesta del chatbot
def generate_response(client, messages, config):
    """Genera respuesta usando el modelo Gemini"""
    try:
        # Convertir mensajes de Streamlit al formato de Gemini
        contents = []
        for msg in messages:
            role = "user" if msg["role"] == "user" else "model"
            contents.append(
                types.Content(
                    role=role,
                    parts=[types.Part.from_text(text=msg["content"])]
                )
            )
        
        # Generar respuesta usando streaming
        response_text = ""
        response_placeholder = st.empty()
        
        for chunk in client.models.generate_content_stream(
            model="gemini-2.5-flash-lite",
            contents=contents,
            config=config,
        ):
            if not chunk.candidates or not chunk.candidates[0].content or not chunk.candidates[0].content.parts:
                continue
            
            response_text += chunk.text
            # Actualizar la respuesta en tiempo real
            response_placeholder.markdown(f"ü§ñ **Asistente:** {response_text}")
        
        return response_text
        
    except Exception as e:
        st.error(f"Error al generar respuesta: {str(e)}")
        return f"Lo siento, hubo un error al procesar tu solicitud: {str(e)}."

# Funci√≥n principal de la aplicaci√≥n
def main():
    st.title("ü§ñ Pro AI Pro")
    st.markdown("---")
    
    # Inicializar cliente
    client = init_gemini_client()
    if not client:
        st.stop()
    
    # Obtener configuraci√≥n
    config = get_model_config()
    
    # Inicializar historial de chat en session_state
    if "messages" not in st.session_state:
        # Incluir mensajes iniciales del ejemplo original
        st.session_state.messages = [
            
        ]
    
    # Sidebar con configuraciones
    with st.sidebar:
        st.header("‚öôÔ∏è Configuraci√≥n")
        
        # Bot√≥n para limpiar historial
        if st.button("üóëÔ∏è Limpiar Historial", type="secondary"):
            st.session_state.messages = []
            st.rerun()
        
        # Informaci√≥n del modelo
        st.markdown("### üìä Informaci√≥n de este Bot")
        st.info("""
        Este bot utiliza el modelo LLM gemini-2.5-flash-lite y cuenta con el contexto de todas las fichas t√©cnicas del √°rea de oferta
        
        """)
        
        # Instrucciones del sistema
        st.markdown("### üìã Como usar este chat?")
        st.text_area(
            "Instrucciones:",
            value="Preguntale como si fuera una persona, se claro con tu pregunta, si bien parece magia pero no lo es. \n\n Si no logras la respuesta correcta, itera y repregunta hasta que lo logres",
            disabled=True,
            height=240
        )
    
    # Mostrar historial de mensajes
    st.markdown("### üí¨ Conversaci√≥n")
    
    # Contenedor para el chat
    chat_container = st.container()
    
    with chat_container:
        for message in st.session_state.messages:
            if message["role"] == "user":
                st.markdown(f"üë§ **T√∫:** {message['content']}")
            else:
                st.markdown(f"ü§ñ **Asistente:** {message['content']}")
        
        st.markdown("---")
    
    # Input para nuevo mensaje
    with st.form(key="chat_form", clear_on_submit=True):
        col1, col2 = st.columns([4, 1])
        
        with col1:
            user_input = st.text_input(
                "Escribe tu mensaje:",
                placeholder="Ejemplo: armame una tabla con 5 productos que sean entrantes...",
                key="user_input"
            )
        
        with col2:
            submit_button = st.form_submit_button("üì§ Enviar", type="primary")
    
    # Procesar nuevo mensaje
    if submit_button and user_input:
        # Agregar mensaje del usuario
        st.session_state.messages.append({
            "role": "user",
            "content": user_input
        })
        
        # Mostrar mensaje del usuario inmediatamente
        st.markdown(f"üë§ **T√∫:** {user_input}")
        
        # Generar respuesta
        with st.spinner("ü§ñ Generando respuesta..."):
            response = generate_response(client, st.session_state.messages, config)
        
        # Agregar respuesta del asistente
        st.session_state.messages.append({
            "role": "assistant",
            "content": response
        })
        
        # Rerun para actualizar la interfaz
        st.rerun()
    
    # Footer
    st.markdown("---")
    st.markdown(
        "<div style='text-align: center; color: #666;'>"
        "üîß Chatbot powered by Google Gemini 2.5 Flash Lite"
        "</div>",
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
